/**
 * Scratch Pad Documents
 *
 * Auto-generated by scripts/sync-scratch-pad.js
 * Source: /Users/omega/Documents/Job Search/LLM Suggestions
 * Generated: 2026-01-14T01:11:42.701Z
 *
 * DO NOT EDIT MANUALLY - Run "npm run sync:scratch-pad" to regenerate
 */

export interface ScratchPadDoc {
  slug: string;
  title: string;
  date: string;
  content: string;
  sourceFile: string;
}

export const scratchPadDocs: ScratchPadDoc[] = [
  {
    "slug": "teleops-mspops-ai-platform-llm-implementation-spec",
    "title": "TeleOps / MSPOps AI Platform – LLM Implementation Spec",
    "date": "2026-01-11",
    "content": "# go ahead.\n\n```markdown\n# TeleOps / MSPOps AI Platform – LLM Implementation Spec\n\n> **Instruction to LLM:**  \n> You are a senior engineer and architect assisting in building a Principal‑level capstone project called **TeleOps / MSPOps AI Platform**.  \n> Follow this spec carefully.  \n> Work **phase by phase**, produce code, configs, and docs per phase, and keep track of architecture and goals across all phases.\n\n---\n\n## 0. Project Overview\n\n### 0.1 Vision\n\nBuild an **AI‑assisted operations platform** for MSPs and telecom operators that:\n\n- Ingests alerts/logs from multiple systems (network, infra, apps).\n- Correlates them into incidents.\n- Uses **domain‑aware LLM + RAG** to:\n  - Generate **root cause hypotheses**.\n  - Recommend **remediation actions** with confidence and risk.\n  - Draft **tickets and comms** (e.g., incident summaries, customer updates).\n- Provides GUIs for:\n  - Synthetic data generation.\n  - Incident triage and RCA.\n  - Observability, metrics, and evaluation.\n\nThis must look like a **small but coherent platform**, not a toy script.\n\n### 0.2 Target Context\n\n- Domain: **Telecom + MSP IT operations** (NOC/SOC style).\n- Environment: Local/dev first (Docker, local DBs), but architecture must be **cloud deployable**.\n- Audience: Principal TPM/PM interviewers at Mag7; assume technical depth.\n\n---\n\n## 1. High‑Level Goals and Non‑Goals\n\n### 1.1 Project Goals\n\nThe platform must:\n\n1. Provide an **end‑to‑end path**:\n   - Synthetic log/alert generation → correlation → incident → RCA → remediation suggestion → ticket/comms.\n2. Use a **telecom‑aware LLM** (e.g., Tele‑LLM or telecom‑tuned Mistral/Llama) via open‑source stack where possible.\n3. Support **RAG over telecom/MSP standards** (3GPP/ETSI/ITU/IETF via preprocessed datasets like TSpec‑LLM) for spec‑driven reasoning.\n4. Expose:\n   - A **backend API** for TeleOps functions.\n   - **GUIs** for:\n     - Synthetic data control.\n     - Incident triage & RCA exploration.\n     - Metrics and observability.\n5. Include **evaluation and metrics**:\n   - RCA accuracy vs ground truth (for synthetic incidents).\n   - Impact on simulated MTTR.\n   - LLM call latency, cost model (even if approximate).\n6. Have clear **governance, safety, and human‑in‑the‑loop** design.\n\n### 1.2 Non‑Goals\n\nThe platform does **not** need to:\n\n- Integrate with real operator data or real ServiceNow/Jira instances.\n- Support high availability or multi‑region DR.\n- Implement full CI/CD; basic automation and scripts are enough.\n\n---\n\n## 2. Architecture and Phasing\n\n### 2.1 Core Components\n\n- **Data Plane**\n  - Synthetic log/alert generator.\n  - Storage for raw logs, correlated incidents, and topology/CMDB.\n- **AI Plane**\n  - LLM interface (Tele‑LLM or similar).\n  - RAG pipeline over telecom/MSP documents.\n  - Multi‑agent workflow for correlation, RCA, and remediation.\n- **Control Plane**\n  - API layer (REST/GraphQL/FastAPI).\n  - Orchestration (LangGraph or similar).\n- **Presentation Plane**\n  - GUIs (Streamlit/Next.js/FastAPI+Jinja) for:\n    - Data generation.\n    - Incident triage & RCA.\n    - Metrics & evaluation dashboards.\n- **Observability & Evaluation**\n  - Metrics store (Prometheus‑like or PostgreSQL tables).\n  - Basic logging and traces.\n\n### 2.2 Phases (to control context size)\n\n> **Instruction to LLM:**  \n> Work sequentially by phases. At each phase, produce:\n> - Design summary.\n> - Code/artefacts.\n> - README update.\n> Keep a short **`docs/progress_log.md`** file updated with what is done and what remains.\n\n- **Phase 1 – Requirements & Domain Model**\n- **Phase 2 – Synthetic Data & Schema**\n- **Phase 3 – Core Backend & Storage**\n- **Phase 4 – LLM + RAG + Multi‑Agent Workflows**\n- **Phase 5 – GUIs (TeleOps Console)**\n- **Phase 6 – Metrics, Evaluation, and Tests**\n- **Phase 7 – Governance, Safety, and Final Docs**\n\n---\n\n## 3. Phase 1 – Requirements & Domain Model\n\n### 3.1 Domain Concepts\n\nDefine and document:\n\n- **Alert**\n  - Fields: `id`, `timestamp`, `source_system`, `host`, `service`, `severity`, `alert_type`, `message`, `tags`, `raw_payload`.\n- **Incident**\n  - Fields: `id`, `start_time`, `end_time`, `severity`, `status`, `related_alert_ids`, `summary`, `suspected_root_cause`, `impact_scope`, `owner`, `created_by`.\n- **Topology/CMDB Entity**\n  - Fields: `id`, `type` (`router`, `switch`, `app`, `service`, `customer_tenant`, etc.), `name`, `env`, `dependencies`, `metadata`.\n- **RCA Artifact**\n  - Fields: `id`, `incident_id`, `hypotheses` (list), `evidence` (alerts, logs, topology), `confidence_scores`, `llm_model`, `timestamp`.\n- **Remediation Recommendation**\n  - Fields: `id`, `incident_id`, `steps`, `risk_level`, `requires_approval`, `rollback_plan`, `generated_by`.\n\n### 3.2 Requirements (LLM should capture in `docs/requirements.md`)\n\n- Ingestion from synthetic generators only (for now).\n- Support **at least 3 incident types**:\n  1. Network degradation (e.g., high packet loss or latency).\n  2. Application outage (HTTP 5xx spike).\n  3. Authentication/identity issue (e.g., auth service failure).\n- Multi‑tenant readiness:\n  - Field like `tenant_id` on alerts/incidents.\n- Human‑in‑the‑loop:\n  - Ability to override RCA and remediation.\n  - Audit log of AI suggestions vs human decisions.\n\n---\n\n## 4. Phase 2 – Synthetic Data & Schema\n\n### 4.1 Storage Schema\n\n- Use **PostgreSQL** (or SQLite for dev) with SQLAlchemy models for:\n  - `alerts`\n  - `incidents`\n  - `topology_entities`\n  - `rca_artifacts`\n  - `remediation_recommendations`\n  - `metrics_events` (for observability).\n- Deliver:\n  - DB migration scripts (e.g., Alembic).\n  - ER diagram (text/PlantUML or Mermaid).\n\n### 4.2 Synthetic Generators\n\nDesign simple, controllable synthetic generators in Python:\n\n- Inputs:\n  - Incident type.\n  - Volume (alerts per minute).\n  - Duration.\n  - Noise level (how many unrelated alerts).\n- Outputs:\n  - Alerts (written into DB).\n  - Ground truth labels:\n    - Incident type.\n    - Root cause description.\n    - Expected remediation steps.\n\n**Trade‑offs (LLM to respect):**\n\n- Focus on **schema realism** and patterns (e.g., bursty alerts, correlated host/service) rather than perfect distribution models.\n- Keep generator deterministic option (seed) for repeatable tests.\n\n### 4.3 Data Sets to Simulate MSP + Telco\n\n- Include alert sources like:\n  - `net-snmp`, `router-os`, `5g-core`, `web-app`, `db`, `auth-service`, `k8s-node`.\n- Tag some incidents as **telecom‑heavy**:\n  - eNodeB/gNodeB degradation.\n  - Core network signaling issues.\n\n---\n\n## 5. Phase 3 – Core Backend & Storage\n\n### 5.1 Tech Stack\n\n- **Backend framework:** FastAPI (simple, async support, OpenAPI).\n- **Database:** PostgreSQL (or SQLite dev).\n- **ORM:** SQLAlchemy.\n- **Task queue (optional but preferred):** Celery/RQ or FastAPI background tasks for async runs.\n\n### 5.2 Services / Modules\n\nImplement as Python packages/modules:\n\n- `teleops.config` – config (env vars, DB URLs, model names).\n- `teleops.models` – ORM models.\n- `teleops.data_gen` – synthetic generators.\n- `teleops.ingest` – API endpoints to start/stop generation.\n- `teleops.incident_corr` – basic correlation engine:\n  - Rule-based: group alerts into incidents based on time window, host, service, and topology relationships.\n- `teleops.api` – FastAPI app:\n  - Endpoints to:\n    - Trigger generation.\n    - List alerts/incidents.\n    - Retrieve RCA artifacts.\n    - Trigger RCA/LLM runs (Phase 4).\n\n### 5.3 Trade‑offs\n\n- Start with **rule-based correlation**; add AI‑augmented correlation only after LLM integration.\n- Prefer **simple modularity** over complex microservices; monorepo, single FastAPI app with clear module boundaries.\n\n---\n\n## 6. Phase 4 – LLM + RAG + Multi‑Agent Workflows\n\n### 6.1 LLM Model Choices\n\nUse **open models**:\n\n- Preferred:\n  - Telecom‑tuned model (e.g., Tele‑LLM, Mistral‑Telco, or Llama‑based telecom model).\n- Infra:\n  - Run via **vLLM**, **Ollama**, or **text-generation-inference** locally.\n- Add adapter to switch to a generic open model if Tele‑LLM not available.\n\n**Trade‑offs:**\n\n- Model size vs latency:\n  - Default to ~3B–8B model for local feasibility.\n  - Document what would change for 70B/hosted models.\n\n### 6.2 RAG Pipeline\n\n- Build a **RAG index** from:\n  - Telecom/MSP spec docs: 3GPP, IETF RFCs, TSpec‑LLM preprocessed text, etc.\n- Use:\n  - `langchain` or `llama_index` or `Haystack` (pick one and justify in comments).\n  - `FAISS` or similar vector store.\n- RAG steps:\n  1. Given an incident, construct a query with key fields (alert types, services, topology).\n  2. Retrieve relevant spec fragments.\n  3. Provide them as context to the LLM for RCA and remediation reasoning.\n\n### 6.3 Multi‑Agent Orchestration\n\nUse **LangGraph** (or another graph‑based orchestrator):\n\n- Agents:\n  - **Pattern Matcher Agent**\n    - Input: incident + related alerts.\n    - Output: structured description of patterns (e.g., fan‑out failures, single‑point, cascade).\n  - **RCA Agent**\n    - Input: incident, pattern description, RAG context.\n    - Output: list of root cause hypotheses with confidence.\n  - **Remediation Agent**\n    - Input: RCA output, topology, known runbook patterns.\n    - Output: remediation steps, risk, rollback suggestions.\n  - **Comms Agent**\n    - Input: incident + RCA/remediation.\n    - Output: incident summary, technical update, customer‑friendly message.\n\n### 6.4 Prompting and Guardrails\n\n- Design **prompt templates**:\n  - System prompts: role + domain (telco/MSP ops).\n  - Few‑shot examples: small set per incident type.\n- Guardrails:\n  - Explicitly instruct model to:\n    - Indicate uncertainty.\n    - Never invent commands that are destructive without labeling risk.\n    - Return **structured JSON** where needed (for parsing).\n\n---\n\n## 7. Phase 5 – GUIs (TeleOps Console)\n\n### 7.1 Tech Stack\n\n- For speed: **Streamlit** or **React/Next.js** (pick based on your comfort; Streamlit is faster to bootstrap).\n- GUIs can talk to backend API (FastAPI) via HTTP.\n\n### 7.2 GUI Philosophy\n\n- Separate flows by **persona**:\n  1. **Ops Engineer Console**\n     - View incidents, drill into alerts and topology.\n     - View AI‑generated RCA & remediation.\n     - Accept/reject/override AI suggestions.\n  2. **Data/Scenario Designer**\n     - Control synthetic data generation.\n     - Configure incident types, volumes, noise.\n  3. **Leadership / SRE Manager Dashboard**\n     - View MTTR, incident volumes, AI vs human RCA accuracy, automation usage.\n- Design for:\n  - **Clarity over aesthetics**.\n  - Explicit trust boundaries: show confidence, risk flags, and whether automation is enabled.\n\n### 7.3 Screens (Minimum)\n\n1. **Incident List + Detail**\n   - Filter by time, severity, status, incident type.\n   - Detail view shows:\n     - Timeline of alerts.\n     - Topology graph snippet (simple) for impacted entities.\n     - RCA hypotheses, confidence, evidence links.\n     - Remediation steps and status.\n\n2. **Synthetic Scenario Builder**\n   - Form to define:\n     - Incident type.\n     - Duration, alert rate, noise.\n   - Button: “Generate Scenario”.\n\n3. **Metrics Dashboard**\n   - Charts: MTTR (simulated), RCA accuracy vs baseline, LLM latency, number of automated remediation actions vs manual.\n   - Basic tables/graphs.\n\n---\n\n## 8. Phase 6 – Metrics, Evaluation, and Test Plans\n\n### 8.1 Performance & Outcome Metrics\n\nTrack at least:\n\n- **Operational metrics:**\n  - Simulated MTTR (baseline vs with TeleOps).\n  - Incident detection to first hypothesis time.\n  - True/false RCA rate (match against ground truth).\n  - Suggested vs accepted remediation rate.\n\n- **System metrics:**\n  - LLM call latency (p50/p95).\n  - Number of tokens per request.\n  - Throughput (incidents/hour processed).\n\n- **Quality metrics:**\n  - Human rating (1–5) on explanation quality (store in DB for manual review).\n  - Hallucination rate (where suggestions contradict known ground truth).\n\n### 8.2 Evaluation Setup\n\n- Use synthetic dataset with ground truth to:\n  - Compute accuracy of RCA (exact match or similarity).\n  - Compare:\n    - Baseline rule‑only RCA vs LLM‑assisted RCA.\n- Optionally:\n  - Align with GSMA Open‑Telco RCA benchmark style for telco incidents (when feasible).\n\n### 8.3 Testing\n\n- **Unit tests:**\n  - Data generators.\n  - Correlation rules.\n  - API endpoints.\n- **Integration tests:**\n  - End‑to‑end: scenario generation → incident creation → LLM RCA → recommendation.\n- **“Red Team” tests:**\n  - Ensure LLM does not recommend clearly unsafe actions without warning.\n  - Check for empty or malformed structured responses.\n\n---\n\n## 9. Phase 7 – Governance, Safety, Docs, and Cadence\n\n### 9.1 Governance & Safety\n\n- Define:\n  - **Roles**:\n    - Ops Engineer.\n    - SRE Manager.\n    - Platform owner.\n  - **Human‑in‑the‑loop gates**:\n    - AI suggestions must be approved for remediation.\n    - Auto‑healing toggle with policy (only low‑risk actions).\n  - **Audit logging**:\n    - Store AI suggestions, human decisions, timestamps, and diffs.\n\n### 9.2 Documentation\n\nProduce:\n\n- `README.md` – high‑level overview and quick start.\n- `docs/architecture.md` – diagrams and rationale.\n- `docs/llm_design.md` – prompts, RAG design, model choice, trade‑offs.\n- `docs/evaluation.md` – metrics, benchmarks, results.\n- `docs/governance.md` – roles, safety, audit log model.\n- `docs/progress_log.md` – short running log of what is implemented.\n\n### 9.3 Cadence Templates\n\nCreate boilerplate markdown templates:\n\n1. **Weekly Status (for stakeholders)** – `templates/weekly_status.md`\n   - Sections:\n     - Summary\n     - What was done\n     - Risks/blockers\n     - Next week\n     - Metrics snapshot\n\n2. **Design Doc One‑Pager** – `templates/design_onepager.md`\n   - Problem statement\n   - Goals / non‑goals\n   - Architecture overview\n   - Risks & mitigations\n   - Open questions\n\n3. **Incident Postmortem Template** – `templates/postmortem.md`\n   - Incident ID\n   - Impact\n   - Timeline\n   - AI vs human RCA comparison\n   - Lessons learned\n\n---\n\n## 10. Constraints & Expectations for the LLM\n\n### 10.1 Constraints\n\n- Optimize for **clarity and completeness**, not production‑grade polish.\n- Prefer open‑source components; if you propose closed ones, provide open alternative.\n- Keep language and code base in **Python** primarily.\n- Maintain a consistent folder structure, e.g.:\n\n```text\nteleops/\n  teleops/\n    __init__.py\n    config.py\n    models.py\n    data_gen/\n    ingest/\n    incident_corr/\n    llm/\n    rag/\n    agents/\n    api/\n  ui/\n    streamlit_app/  (or react_app/)\n  docs/\n  templates/\n  tests/\n  README.md\n```\n\n\n### 10.2 How to Work with This Spec\n\n> **Instruction to LLM (for future runs):**\n\n1. **Start with Phase 1**, produce the required docs and base folder structure.\n2. After each phase:\n    - Update `docs/progress_log.md`.\n    - Make sure previous phases’ contracts are not broken.\n3. Do not rewrite the spec; reference it.\n4. When context window is tight:\n    - Re‑load only relevant sections of this spec for the current phase.\n    - Keep the architecture and goals consistent with the earlier work.\n\n---\n\n## 11. Success Criteria (Principal‑Grade)\n\nThe project will be considered successful if:\n\n- There is a **working demo**:\n    - Generate scenario → see incidents → run AI RCA → see recommendations → inspect metrics.\n- The repo clearly communicates:\n    - Problem framing (TeleOps/MSPOps).\n    - System architecture and trade‑offs.\n    - AI design (models, RAG, prompts).\n    - Metrics and evaluation results.\n    - Governance and safety thinking.\n- A Principal TPM/PM reviewer can see:\n    - Clear business/ops value (MTTR, triage time).\n    - End‑to‑end ownership thinking.\n    - Explicit trade‑offs and future roadmap.\n\n> **Next step for LLM:**\n> Begin with Phase 1: create `README.md`, `docs/requirements.md`, and `docs/progress_log.md` with the domain model and requirements from this spec, then proceed phase by phase.\n\n```\n```\n\n",
    "sourceFile": "MSPOps using AI.md"
  },
  {
    "slug": "system-design-practice-problems",
    "title": "System Design Practice Problems",
    "date": "2026-01-08",
    "content": "# Okay. So, for system design - suggest me what I should start with to harden my muscle. Start with simple to increasingly complex. Ensure that all these would make me a standout for Principal TPM role at Mag7 company. For now, give me list where I can start with.\n\nit would be great if these are related to my work before but that is not a hard requirement. confirm your understanding before responding. Think through before you respond.\n\nUnderstanding is:\n\n- You want a **concrete list of system design problems**, ordered from simple → complex.\n- They should **push you toward Principal TPM bar at MAG7**, not generic LC questions.\n- Ideally they **leverage your real background** (monitoring, payments, fraud, large IT ops), but that’s optional.\n- You’ll use this list as the backbone of your daily/weekly design practice.\n\nBelow is the list only (no long commentary), structured by level and aligned to your profile.\n\n***\n\n## Level 1 – Warm‑up (core patterns)\n\n1. **Centralized Log Aggregation \\& Search for a Telco**\n    - Collect logs from hundreds of services; enable search, alerting, and retention.\n2. **API Rate Limiter for Consumer Apps**\n    - Per‑user, per‑IP, per‑API rate limits with burst handling and backpressure.\n3. **Idempotent Payments Webhook Handler**\n    - Process PSP webhooks (auth/capture/refund) exactly once, with retries and out‑of‑order events.\n4. **Feature Flag Service for Multiple Products**\n    - Roll out features by cohort, region, and percentage, with safety rollbacks.\n5. **Simple Real‑Time Notification System**\n    - Send email/SMS/push notifications with retries and dead‑letter handling.\n\n***\n\n## Level 2 – Directly aligned to your background\n\n6. **E2E Monitoring \\& Alerting Platform for an MSP**\n    - Multi‑tenant metrics, dashboards, and alert rules; SLOs per customer; oncall workflows.\n7. **Payment Aggregator for a Telco**\n    - Multiple PSP integrations, unified API, routing, retries, reconciliation, and reporting for \\$500M+/year flow.\n8. **Subscription Billing Platform for Telco Services**\n    - Recurring billing, proration, dunning, suspensions, and integration with payment aggregator.\n9. **Real‑Time Fraud Decisioning Service (v1 Rules‑Only)**\n    - Sits on auth path; velocity rules, blocklists, geo checks; sub‑50ms P99; evidence logging.\n10. **Chargeback \\& Dispute Management System**\n    - Link chargebacks to auths, store evidence, track deadlines, and status across card networks.\n\n***\n\n## Level 3 – Principal‑level (cross‑org, cross‑domain)\n\n11. **Global SLO \\& Error Budget System for a Large Org**\n    - Define, track, and enforce SLOs across hundreds of services; error budgets drive release policies.\n12. **Multi‑Region Active‑Active Payment \\& Fraud Stack**\n    - Consistent auth decisions across regions; failover; data residency; network partitions.\n13. **Enterprise‑Wide Event Bus for Risk \\& Compliance**\n    - Standardized risk events (auth, login, device, content); consumers: fraud, security, analytics, compliance.\n14. **Unified Evidence \\& Audit Platform**\n    - Aggregates logs, metrics, decisions, and tickets into immutable evidence for audits and regulatory requests.\n15. **ML‑Augmented Fraud Platform (v2)**\n    - Hybrid rules + ML: feature store, model serving, shadow/challenger, drift monitoring, rollback.\n\n***\n\n## Level 4 – “Stand‑out” Principal TPM scenarios\n\n16. **Company‑Wide Incident Management \\& Postmortem System**\n    - From detection → paging → Slack/war room → postmortem templates → follow‑through tracking, across orgs.\n17. **Cost‑Optimized Observability Platform Migration**\n    - Migrate from expensive vendor to in‑house stack with zero blind spots, clear SLOs, and staged rollout.\n18. **Unified Risk Platform Across Products**\n    - Single risk profile per user/tenant/device across payments, logins, content, and support; shared scoring and policies.\n19. **Customer‑Facing Trust \\& Safety Portal**\n    - Users view and dispute transactions, see security events, and control risk settings; integrates with fraud and billing.\n20. **Regulatory‑Grade Data Retention \\& Deletion System**\n    - GDPR/CCPA‑compliant retention, legal holds, and right‑to‑be‑forgotten flows across payment and monitoring data.\n\n***\n\nUse this as your practice ladder:\n\n- Start with 1–5 to lock structure.\n- Then 6–10 (directly your lane).\n- Then 11–15 for Principal‑tier signal.\n- 16–20 only after you’re comfortable; these are the “this person thinks like L6+” problems.\n\n",
    "sourceFile": "Okay. So, for system design - suggest me what I sh.md"
  },
  {
    "slug": "principal-tpm-interview-prep-assessment",
    "title": "Principal TPM Interview Prep Assessment",
    "date": "2026-01-07",
    "content": "# Here are my answers :\n\nA. Target and constraints\n\n    Which companies/teams are you actively targeting in the next 3–6 months (e.g., Google Payments, Apple Security, Meta Infra)? - Google is what I want, but anything works. Open to all.\n    \n    What level are you aiming for (e.g., L7 Google, Principal at Meta, Principal at Apple)? - L6 at google.\n    \n    How much time per weekday and weekend can you realistically invest (hours/day)? - I dont have  a job now, badly need it. so, go with 60 hours/wk is doable.\n    B. Current strongest stories (now, not theoretical)\n\n    List your top 3 programs you can talk about in interviews (1–2 bullets each: scope, #teams, impact). \n    1. pitching, designing and implementing a comprehensive e2e monitoring platform at Altice USA in 8 months, a large MSP in USA. \n    2. Managed a team of 130+ professionals (USA and India) in managing IT operations for Altice USA. Budget planning, resource planning, managing SLAs, updating customer EVP and CTO etc. \n    3. Implemented Payment Aggregator platform for Altice USA. Low cost, reliable solution that handled $500M payments annually. \n    \n    For each, what is the headline you would claim (e.g., “Cut fraud losses by X”, “Shipped Y at Z scale”, “Unblocked multi‑org migration”)?\n    1. Transforming Altice USA IT monitoring from reactive to Proactive in record time, on budge and ontime. \n    2. Managed large (130 professionals) IT Teams across 3 continents and 5 time zones with $80M budget. \n    3. Reliable and Scalable payment platform at lowest cost. \n    4. Well aware of PCI/PII compliance, SOC - type 2 audits having completed audits for years. \n    C. Technical and domain readiness (today)\n\n    On a 1–5 scale (5 = strong Principal, 3 = solid Senior), rate yourself today on:\n    \n    System design & architecture discussions. - I am at 2.5 but probably 3.5 for TPM expectations, this is my next target for improvement.\n    \n    Fraud/payments domain depth. - I am at 4 or 4.5. I know most internal workings of payments having implemented a payment aggregator platform from scratch for Altice USA.\n    \n    Infra/SRE/SLO concepts.- I am at 5. I managed a large team supporting IT operations and Development for Altice USA in Sr Director position. \n    \n    Data/ML systems (not modeling, but pipelines, features, deployment). - I am at 3.5 for a TPM expectations. as a Sr Director, I understand Data flows and AI systems for a TPM. \n    Add 1 sentence each on why you picked that number.\n\n    What technical topics do you dread getting in interviews (e.g., consistency models, queues/backpressure, ML infra, security/privacy, networking basics)?\n    Mostly ML infra and consistency models. \n    D. Interview‑critical skills\n\n    Behavioral: how comfortable are you with Amazon‑style bar raiser questions (ownership, conflict, failure) and giving sharp, metric‑backed stories (1–5)? - 3.5 I say, I dont have many stories. Most of my work go un noticed. I work to avoid production incidents. With the 2-3 stories I have as bar riser, I am very comfortable at 4 score. \n    \n    \n    Exec communication: how comfortable are you with reading a system and then summarizing it for a VP in 3–4 minutes (1–5)? - 3 i should say. I can put forward all the necessary info, I struggle with structure. So I would say things but then might add few at the end. \n    \n    Design interview: what exactly tends to go wrong when you do a mock system design (scope, structure, technical depth, getting lost, time)? - Time, I might take more time to design it.\n    E. Mindset / patterns that can hurt you now\n\n    What do you tend to overdo: detail, architecture depth, boiling the ocean, or playing it too safe? - architecture depth might be shallow. I am good at Risks, mitigation, reporting, compliance, technical depth etc. \n    \n    What have past managers / peers actually said that stung but might be true (e.g., “overcomplicates”, “slow to decide”, “too theoretical”, “pushes too hard”)? - inadequate followup. I am good at planning, but not great at follow up. I expect that from my team to bring up issues. \n    F. Assets you can immediately leverage\n\n    What is already ready to show now (fraud platform docs, code, dashboards, blog posts, etc.) and what’s realistically 2–3 weeks out? - Everything is ready except my confidence in structuring answers to Interview questions. it includes stories of good/bad, technical explanation, etc. \n    \n    Do you have senior people who can give real referrals at MAG7 companies in the next 1–2 months? - yes, I do. Including Customer CTO/EVPs\n    You’re close but uneven: your domain/execution are at or above L6 bar; your visible system design + interview packaging are not. The plan below is 8–10 weeks, assuming 60 hrs/week, focused on what moves the needle for L6 at Google or similar.[^1][^2]\n\n***\n\n## 1. Where you stand (blunt assessment)\n\nStrengths (you should lean hard on these):\n\n- **Scale \\& responsibility already at Principal level**\n    - Ran 130+ people, \\$80M budget, multi‑continent, exec‑facing. This is rare for L6 candidates and is a big credibility wedge.[^3][^4]\n- **Domain**\n    - Payments + PCI/PII/SOC2 + having built a \\$500M/year aggregator from scratch is very strong for Google/Apple/Stripe/Fintech teams.[^5][^6][^1]\n- **Ops / SRE / monitoring**\n    - E2E monitoring platform, real SLO/SLA ownership, production avoidance mindset—that’s exactly what infra/security/fraud orgs want.[^7][^8][^9]\n\nGaps (these will block you if not fixed):\n\n- **System design interview performance (you rated 2.5, time overrun)**\n    - You’ll get at least one hard design round. Today you risk “Senior TPM, not clearly L6” feedback.\n- **Answer structure under pressure**\n    - You know the content but admit structure is weak and you add things at the end. That’s exactly how you lose bar-raiser and hiring committee.\n- **Follow‑through signal**\n    - “Inadequate followup” is a red flag for Principal; at interview it shows as: good plan, but thin on “how you drove it to completion”.\n\nYou do **not** need 12 months. You do need 8–10 very disciplined weeks.\n\n***\n\n## 2. Targeted plan (8–10 weeks, parallel tracks)\n\n### Track A – System design muscle (3–4 weeks heavy, then maintain)\n\nGoal: reliably do one L6‑caliber system design in 40–45 minutes with clear structure and enough technical depth.\n\n1) **Fix your template, not your raw knowledge**\nFor every design, use the exact same spine:\n    - Clarify requirements (functional + scale + latency + availability + compliance).\n    - Propose high‑level architecture (boxes/arrows).\n    - Drill into 2–3 key components (API + data model + scaling).\n    - Talk about failure modes, SLOs, and evolution (v1 → v2).\nDo this even if interviewer doesn’t ask; they’re grading that structure.[^10][^1]\n2) **Practice in your domain first, then generalize**\n    - Design exercises for you:\n        - Real‑time fraud decisioning system (your project, but from scratch in 45 min).\n        - Payment aggregator for a telco at \\$1B/year.\n        - Multi‑region monitoring/alerting platform with SLOs.\n    - Once you’re comfortable: add 2 generic prompts (URL shortener, notification system) to show breadth.\n3) **Daily reps**\n    - 1 design **per day** on paper/whiteboard, 30–35 min hard stop, 10 min self‑critique:\n        - Did you cover requirements, data, APIs, scale, failure modes, evolution?\n    - Twice a week, do a live mock with a human (use your referrals / peers).\n\nYou don’t need to become an SDE; you need to show Staff‑level system thinking and discipline.[^4][^11]\n\n***\n\n### Track B – Behavioral / bar‑raiser story hardening (3–4 weeks)\n\nGoal: 8–10 sharp stories, each 3–5 minutes, metric‑backed, reusable across questions.\n\n1) **Lock a simple story format (STAR but ruthless)**\n    - Situation (1–2 sentences).\n    - Task (your responsibility, not “we”).\n    - Actions (3–5 bullets, focusing on decisions and conflict).\n    - Result (metrics, what changed, what you’d do differently).[^2]\n2) **Mine your 3 big programs for more angles**\nFrom each of:\n    - Monitoring platform.\n    - 130‑person multi‑geo ops.\n    - Payment aggregator.\nExtract specific stories for:\n    - Massive ambiguity / no clear owner.\n    - Conflict (with EVP/CTO, vendors, internal teams).\n    - Failure or near‑miss (incident, delay, bad decision) and how you recovered.\n    - “Think big” and cost/efficiency wins (your low‑cost payment platform is a strong example).\n3) **Make them metric‑tight**\nFor each story, force numbers:\n    - X systems, Y teams, Z time, impact: dollars, % incidents, % availability, % cost reduction, etc.[^1]\n4) **Drill delivery**\n    - Record yourself answering 2–3 behavioral questions daily, watch, and trim.\n    - Aim for **3–4 minutes max** per story; if you ramble, cut detail, not the arc.\n\n***\n\n### Track C – Exec narrative / comms (2–3 weeks layered in)\n\nGoal: you talk like someone who has already been at L6 for a while.\n\n1) **Standardize your 3–4 minute “who I am” pitch**\n    - One for recruiters.\n    - One for hiring managers.\n    - One for VP/executive loops.\nContent:\n    - 1 line background.\n    - 2–3 big outcomes (monitoring, payments, operations).\n    - 1 line on fraud platform as your current technical capstone.\n    - 1 line on what you want to own next (fraud/infra/SRE‑type programs at their company).\n2) **Practice “VP summaries” of systems**\n    - Take your fraud platform, payment aggregator, and monitoring system.\n    - For each, practice a 3‑minute summary hitting:\n        - Business problem, stakes.\n        - What you built + key design decisions (no low‑level tech laundry lists).\n        - Measurable outcome + what you’d do next.\n\nThis directly addresses your “structure” weakness.\n\n***\n\n### Track D – Plugging specific knowledge holes (ongoing, but scoped)\n\nYou said weaknesses: **ML infra** and **consistency models**.\n\nYou don’t need depth like an ML engineer; you need a **TPM‑appropriate baseline**.\n\n1) **Consistency / queues / backpressure (1 week, evenings)**\n    - Read 1–2 good system design posts on:\n        - Strong vs eventual consistency; leader–follower replication.\n        - Idempotency and exactly‑once semantics (you already use this in your docs).[^3]\n    - Apply it to your fraud platform design:\n        - How do you ensure consistent decisions if Redis or DB lag?\n        - Where eventual consistency is acceptable (evidence, analytics).\n2) **ML infra basics (1 week, evenings)**\n    - Focus on:\n        - Feature store, training vs serving skew, model deployment patterns (shadow, canary, AB).\n        - Monitoring: drift, performance, rollback.[^12]\n    - You already designed most of this in your fraud project; just align your mental model with standard ML platform concepts.[^13][^14]\n\nYour fraud docs + a couple of focused reads are enough to answer ML‑infra‑ish questions credibly.\n\n***\n\n## 3. How to present yourself now\n\nIn all conversations (recruiter, HM, referrals), steer toward this framing:\n\n- “Senior Director‑level operations and payments background, targeting L6 TPM roles in fraud, payments, and infra.\nBuilt a \\$500M/year payment platform, an E2E monitoring stack, and now an enterprise‑grade fraud decisioning platform as a hands‑on capstone that reflects L6 technical depth.”[^8][^5][^7]\n\nYour **angle**: “already did the org‑scale, now proving I can play at Staff/Principal technical bar with this fraud platform and system design discipline.”\n\n***\n\n## 4. What to **ignore** for now\n\nGiven 8–10 weeks and 60 hrs/week, skip:\n\n- Deep ML modeling theory.\n- Extra side projects.\n- Perfecting every detail in your docs; they’re already more detailed than most candidates.[^13][^8][^3]\n\nAll energy should go into:\n\n- Making your **existing work legible** to interviewers.\n- Getting your system design and behavioral answers **tight and repeatable**.\n\nIf you want, next step can be: you paste one behavioral story and one design outline; can mark up exactly where you’re losing points and show a “fixed” version.\n<span style=\"display:none\">[^15][^16][^17][^18][^19][^20][^21][^22][^23][^24][^25][^26][^27][^28][^29][^30][^31]</span>\n\n<div align=\"center\">⁂</div>\n\n[^1]: https://www.mariogerard.com/tpm-at-google-preparing-for-tpm-interviews-at-amazon-microsoft-google-part-iii/\n\n[^2]: https://amazon.jobs/content/en/how-we-hire/tpm-interview-prep\n\n[^3]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/f0437ff1-75b3-4b90-8cf4-87b01fbc7ff9/01-technology-stack-architecture.md\n\n[^4]: https://dropbox.github.io/dbx-career-framework/ic6_principal_technical_program_manager.html\n\n[^5]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/ed572a69-a273-41d6-a7fc-78e83621eccb/02-entities-schemas-features.md\n\n[^6]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/a3910faf-7679-4552-b97d-13911ceeed73/04-evidence-disputes-economics.md\n\n[^7]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/b810c776-ce7a-4273-952a-7a284bb337f4/08-load-testing.md\n\n[^8]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/84b04468-4645-4760-8fee-68708f2b4e32/architecture.md\n\n[^9]: https://sre.google/sre-book/service-level-objectives/\n\n[^10]: https://www.tryexponent.com/guides/amazon-tpm-interview\n\n[^11]: https://www.mariogerard.com/depth-vs-breadth-vs-swat-technical-program-managers/\n\n[^12]: https://www.harness.io/harness-devops-academy/what-is-a-service-level-objective-slo\n\n[^13]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/e9bc0969-e77f-4f2b-9738-cac4dcb4dbdb/03-detection-logic-policy-engine.md\n\n[^14]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/aa788bea-6bac-424c-8fef-9be1eab43dcc/05-testing-monitoring-checklist.md\n\n[^15]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/9350e396-a28f-4300-b163-cd8fab8d1929/architecture.html\n\n[^16]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/e813d392-04e6-47a5-a4b5-58b4c40a64a0/07-gui-dashboard.md\n\n[^17]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/0898eebc-fa93-4aeb-8a27-0815beb88766/api-reference.md\n\n[^18]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/256e8de4-d562-40c3-855f-44fbaacbc21c/01-technology-stack-architecture.html\n\n[^19]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/3960c498-aaae-47a8-9b5e-fc09a7fcd582/03-detection-logic-policy-engine.html\n\n[^20]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/587a3cb7-d8f8-488f-aec0-3cf63f1dfda0/05-testing-monitoring-checklist.html\n\n[^21]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/5dda8345-8187-4425-a1c4-27b34583f554/04-evidence-disputes-economics.html\n\n[^22]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/9944ef4f-e393-4b1f-ab98-0112ab70b995/overview.html\n\n[^23]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/bf4c6119-3451-4166-a3e9-6181e921c692/getting-started.html\n\n[^24]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/5a26073f-378b-4f0d-a5d8-a86722ff741f/06-sprint-1-implementation.html\n\n[^25]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/564e080d-11d5-4ee2-bbff-7076c69334b3/demo-dashboard.html\n\n[^26]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/54969781-0a38-4ae4-9bd0-6a7ce42e7347/demo-dashboard.md\n\n[^27]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/7a5e0a77-9a30-499d-b38f-17488c1de53e/06-sprint-1-implementation.md\n\n[^28]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/3d8d8dba-1e53-4929-af88-997f343ef11d/api-reference.html\n\n[^29]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/aa6b2577-9633-4610-b809-8e924643367e/07-gui-dashboard.html\n\n[^30]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/551c8c6b-2549-4a8a-b864-ac64606cb80f/08-load-testing.html\n\n[^31]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/1010217/d9bf7f97-0f48-473e-b606-ef1f9e2f3acf/02-entities-schemas-features.html\n\n",
    "sourceFile": "Here are my answers _ __A. Target and constraints.md"
  },
  {
    "slug": "work-session-guardrails",
    "title": "Work Session Guardrails",
    "date": "2026-01-07",
    "content": "# great, give me checklist i can adhere to so i dont divert from the target. this makes sense and I am glad I asked this.\n\nGood. Treat this like a runbook, not a suggestion.\n\n## Daily / Work-session guardrails\n\n- Before you start: write the **one strategic outcome** for this session (e.g., “Draft VP 1-pager problem + options section”). No UI/visual work until it’s done.\n- Timebox: minimum **60–90 minutes** of deep work on strategy/narrative before touching anything aesthetic.\n- End-of-session check: “What moved me closer to a Principal TPM artifact vs. a prettier site?” If the answer is “CSS,” you course-correct next session.\n\n\n## VP / Exec narrative checklist\n\nFor any 1-pager you create (fraud platform or others), ensure you have:\n\n- Clear **business problem** in VP language (revenue, risk, cost, customer trust; no feature talk).\n- Explicit **stakes**: what happens if we do nothing in 12–24 months.\n- **Audience-aware framing**: is this written for VP Eng, VP Risk, or CFO? Pick one and tune it.\n- 2–3 **real options** with tradeoffs (not strawmen).\n- A crisp **recommendation** with what you are **not** doing and why.\n- Top 3 **risks / unknowns** and how you will de-risk them.\n- Clear **asks**: decisions, resources, or cross-org support needed.\n\nUse LLM to polish wording only *after* each bullet above is thought through.\n\n## LLM usage rules (so it stays your thinking)\n\n- You must write the **bullet skeleton** yourself (problem, options, recommendation, risks).\n- LLM can:\n    - Turn bullets into prose.\n    - Generate variants for different personas (Eng vs Risk vs CFO).\n    - Attack your doc from another angle (“poke holes as a skeptical CFO”).\n- You cannot:\n    - Ask it “Write a VP 1-pager about my fraud platform” from a blank slate.\n    - Ship text you don’t fully understand or can’t defend live.\n\n\n## Aesthetics / website rules\n\n- Fixed budget: max **30 minutes, twice a week** for visual tweaks.\n- Maintain a **design backlog**: every time you feel like tweaking, add it to the backlog instead of doing it.\n- You only pull from the design backlog **after** a strategic artifact is completed (VP 1-pager, roadmap page, decisioning narrative, etc.).\n\n\n## Principal TPM habit checklist\n\nReview this weekly:\n\n- Did I create or improve at least **one** executive-level artifact (1-pager, roadmap, tradeoffs doc)?\n- Did I practice at least **3 reps** of a **60–90 sec spoken** explanation of the fraud platform for different personas (Eng, Risk, CFO)?\n- Did I drive at least one **clear decision** (even if just on paper: “Given X/Y, I recommend path B”)?\n- Is my LLM usage making my thinking **sharper and faster**, or am I hiding behind it?\n\nIf you want, next step: pick *one* artifact (VP 1-pager, decisioning walkthrough, or v2/v3 roadmap), and a concrete checklist for that specific doc can be drafted so you just execute.\n\n",
    "sourceFile": "great, give me checklist i can adhere to so i dont.md"
  },
  {
    "slug": "principal-tpm-at-mag7-companies-reality-check",
    "title": "Principal TPM at Mag7 Companies - Reality Check",
    "date": "2026-01-07",
    "content": "# Principal TPM at Mag7 Companies - Reality Check\n\n## Core Role\nYou're the \"air traffic controller\" for 3-5 cross-org programs simultaneously. Not coding, not managing people directly—you're the connective tissue between engineering, product, data science, infra, legal, and execs. Your job is **unblocking, translating, and de-risking** at scale.\n\n## 70% Time Spent On\n1. **Cross-functional alignment** - Getting 8 teams who don't report to you to ship on time\n2. **Risk mitigation** - Finding what'll blow up 6 months out (compliance, scale, tech debt)\n3. **Exec communication** - Monthly business reviews, OKR tracking, crisis management\n4. **Roadmap sequencing** - Trade-off decisions when 5 orgs want different things\n\n## Frameworks (Real Usage)\n- **DACI/RACI** - Every major decision documented (you're the 'D' or 'A')\n- **PRFAQs/6-pagers** (Amazon) or Design Docs - You write or force teams to write these\n- **OKRs** - Quarterly goals you're measured on (lagging indicators like \"shipped X\" not fluff)\n- **RAID logs** - Risks, Actions, Issues, Dependencies tracking weekly\n- **RTB (Return to Business)** - Post-mortems that actually change behavior\n\n## Tools (Daily Use)\n- **Jira/Linear** - 50+ issues tracked across teams\n- **Confluence/Notion/Google Docs** - Single source of truth docs\n- **Slack** - 30% of your day is unblocking via DMs\n- **Spreadsheets** - Resource modeling, timeline tracking, cost projections\n- **Gantt charts/Airtable** - When dependencies get hairy\n- **BigQuery/Looker/Tableau** - Pull your own metrics, don't wait for data team\n\n## Knowledge Depth Expected\n- **80% understanding** of the tech stack - Can read code reviews, debate architecture, but not implementing\n- **Deep domain expertise** in 1-2 areas (ML infra, payments, ads, cloud platforms)\n- **Business fluency** - Unit economics, P&L impact, competitive landscape\n- **Legal/compliance basics** - GDPR, SOC2, data residency (especially for AI/data products)\n- **Vendor/partner management** - Negotiated $M+ contracts before\n\n## Hiring Manager Mindset\nThey want someone who:\n\n1. **Has scar tissue** - Shipped products that failed/succeeded at scale, knows what breaks\n2. **Doesn't need hand-holding** - Ambiguous problem? You define it, scope it, ship it\n3. **Influences without authority** - VP Engineering ignores you? You find leverage (data, execs, business case)\n4. **Thinks in systems** - \"If we do X for team A, what breaks for teams B, C, D?\"\n5. **Bias to action** - Writes the first draft, calls the meeting, doesn't wait to be asked\n6. **Comfortable with conflict** - Will tell a Director their timeline is fantasy with receipts\n\n## Red Flags They Screen For\n- Only worked in one domain (can't transfer thinking)\n- Too hands-off (wants to \"strategize\" not execute)\n- Too hands-on (still wants to code instead of multiply impact)\n- Poor communication artifacts (rambling docs, no exec summaries)\n- Can't show measurable impact ($X revenue, Y% efficiency, Z quarter early delivery)\n\n## What You Might Miss\n\n### 1. Political Navigation\n- 40% of your job is managing egos, turf wars, and org politics\n- You'll be the \"bad guy\" killing pet projects with data\n- Need to know who actually makes decisions vs. who has the title\n\n### 2. Career Ceiling Reality\n- Principal TPM is often the terminal level before you pick: \n  - Go GPM (Group PM) - people management\n  - Go Director/Head of TPM - org building\n  - Lateral to Product/Eng leadership\n- No clear path to VP without switching tracks at most Mag7\n\n### 3. Comp Structure\n- Base ~$230-280K, RSUs push TC to $450-650K+\n- Stock refreshers heavily tied to impact ratings (top 10% performers)\n- Leveling fights are real - L6 vs L7 is $200K+ difference\n\n### 4. Interview Gap\n- You'll need to show **failure stories** - \"What shipped late and why was it your fault?\"\n- Expect system design for TPMs - \"Design Uber's dispatch system\" with trade-offs\n- Behavioral bar is higher than Senior - they want multi-year strategic thinking\n\n### 5. Lifestyle Trade-offs\n- 50-60 hr weeks standard, especially during launch cycles\n- Meetings 9am-6pm, execution work happens nights/weekends\n- Oncall for programs (yes, TPMs get paged when shit breaks)\n\n### 6. Domain Matters More Than You Think\n- AI/ML TPM roles want ML papers/model knowledge, not just infra\n- Ads TPM needs auction theory and privacy law depth\n- Payments TPM without fintech background won't get callbacks\n\n## Harsh Reality\n70% of your value is **preventing disasters no one sees**. You won't get credit for the outage that didn't happen or the legal issue you caught early. Your wins are often invisible. If you need constant recognition, this isn't the role.\n\nThe best Principal TPMs are former ICs who loved the strategy game more than the coding game, or former Sr. TPMs who proved they can operate 3 levels above their paygrade.\n\n## Critical Self-Assessment Question\n**\"Am I okay being highly paid but rarely the decision-maker, always the decision-enabler?\"**\n\nIf you need to own the \"what,\" go PM. Principal TPM owns the \"how\" and \"when\" at massive scale.\n",
    "sourceFile": "principal_tpm_reality_check.md"
  },
  {
    "slug": "principal-tpm-system-design-learning-path-2-weeks",
    "title": "Principal TPM System Design Learning Path (2 Weeks)",
    "date": "2026-01-07",
    "content": "# Principal TPM System Design Learning Path (2 Weeks)\n\n**Format:** Each exercise = 45-60 min whiteboard session. Solo practice: record yourself explaining out loud.\n\n---\n\n## **WEEK 1: Foundations + Building Blocks**\n\n### **Days 1-2: Learn System Components (30% Theory)**\n\n**Goal:** Understand building blocks well enough to design programs around them.\n\n**Study (3-4 hours total):**\n1. Watch: \"System Design Primer\" on YouTube (Gaurav Sen or ByteByteGo)\n2. Read: CAP theorem, load balancing, caching strategies, message queues, databases (SQL vs NoSQL)\n3. Focus: **What can break?** Not how to code it.\n\n**Practice Problems (Type A - Simple):**\n\n#### 1. Design a URL Shortener (bit.ly)\n- TPM lens: What teams needed? (Backend, Frontend, Infra, Analytics)\n- What breaks first at scale? (Database writes, cache invalidation)\n- Rollout phases? (Internal beta → 10% → 50% → 100%)\n\n#### 2. Design a Rate Limiter\n- TPM lens: Where does it live? (API gateway vs service-level)\n- Dependencies: Who owns gateway? Who sets limits?\n- Risk: What if rate limiter itself fails?\n\n#### 3. Design a Notification Service\n- Email, SMS, Push notifications\n- TPM lens: 3 separate teams (Email infra, SMS vendor, Mobile platform)\n- Sequencing: Can you launch email-only first?\n\n---\n\n### **Days 3-4: Program Design Basics (Type B - Pure TPM)**\n\n**Practice Problems:**\n\n#### 4. Migrate 200 Services from Monolith to Microservices\n- Identify: Which services first? (Leaf nodes with fewest dependencies)\n- Create: 6-month phased plan with milestones\n- RACI: Who owns API contracts? Testing? Rollback?\n- Risks: What if Team X is understaffed?\n\n#### 5. Deprecate a Legacy Internal API (Used by 30 Teams)\n- Discovery phase: Survey all consumers (2 weeks)\n- Migration timeline: 6 months with forced cutoff\n- Communication plan: Monthly emails, Slack channels, office hours\n- Risk: What if 5 teams refuse to migrate?\n\n#### 6. Launch Dark Mode Across 4 Platforms (iOS, Android, Web, Desktop)\n- Dependencies: Design system team must ship tokens first\n- Sequencing: Can iOS ship before Android?\n- Success metrics: Adoption rate, crash rate, user feedback\n- Risk: What if design team is blocked?\n\n---\n\n### **Days 5-7: Hybrid Problems (A + B Combined)**\n\n**Practice Problems:**\n\n#### 7. Design + Launch Real-Time Analytics Dashboard\n- System: Kafka → Flink → PostgreSQL → React frontend\n- Program: 4 teams (Data Eng, Backend, Frontend, Infra)\n- Phases: Phase 1 = Batch (daily), Phase 2 = Near real-time (5 min), Phase 3 = Real-time\n- TPM Questions:\n  - Can you ship Phase 1 in 3 months to unblock stakeholders?\n  - What if Kafka cluster isn't ready? Fallback plan?\n  - How do you validate data accuracy before launch?\n\n#### 8. Design + Launch Search Feature (Like Google/Spotify Search)\n- System: Elasticsearch, ranking ML model, autocomplete\n- Program: ML team (model), Backend (indexing), Frontend (UI), Data (offline eval)\n- TPM Questions:\n  - ML model needs 3 months training. Can you ship basic keyword search first?\n  - How do you A/B test ranking changes safely?\n  - What if search breaks during Black Friday?\n\n#### 9. Design + Launch Video Streaming Platform (Like YouTube/Twitch)\n- System: CDN, transcoding pipeline, adaptive bitrate, chat\n- Program: 6 teams, 12-month timeline\n- TPM Questions:\n  - What's MVP? (Upload + playback only, no chat/comments)\n  - Transcoding is bottleneck (24 hours). How do you parallelize?\n  - Storage costs $500K/month. Who approves budget?\n\n---\n\n## **WEEK 2: Principal-Level Complexity**\n\n### **Days 8-10: Multi-Org, High Ambiguity (Principal-Level)**\n\n**Practice Problems:**\n\n#### 10. Migrate Entire Company to New Cloud Provider (AWS → GCP)\n- 500 services, 50 teams, 18-month timeline\n- TPM Questions:\n  - Sequencing strategy: By team? By service criticality? By dependency graph?\n  - Governance: Weekly steering committee, monthly exec reviews\n  - Risks: Data egress costs ($2M), compliance (SOC2 re-certification), knowledge gaps\n  - Rollback: What if GCP has major outage 6 months in?\n\n#### 11. Launch AI/ML Feature Requiring Model Retraining Pipeline\n- Fraud detection (your domain!), credit scoring, or recommendation engine\n- System: Training pipeline (Airflow), feature store, model serving (TorchServe), monitoring\n- Program: ML Eng team, ML Ops, Backend, Data Eng, Product, Legal (bias review)\n- TPM Questions:\n  - Model training takes 2 weeks. How do you iterate faster?\n  - False positive rate causes support tickets spike. What's your plan?\n  - Legal says model is biased after launch. How do you roll back?\n\n#### 12. Deprecate Entire Legacy Platform (Used by 1M Users)\n- Example: Sunset old mobile app, force users to new app\n- Program: 12 months, user migration, support ramp-down\n- TPM Questions:\n  - Migration phases: Freeze new features → announce sunset (6 months notice) → force upgrade\n  - Communication: In-app banners, emails, push notifications\n  - Risk: 10% of users refuse to migrate. What's the plan?\n\n---\n\n### **Days 11-12: Telecom-Specific (Leverage Your Background)**\n\n**Practice Problems:**\n\n#### 13. Design + Launch Real-Time Fraud Detection System (Your Capstone!)\n- System: Kafka (call records) → Flink (rules engine) → Redis (cache) → Neo4j (graph analysis)\n- Program: Data Eng, ML Eng, Fraud Ops, Legal, Finance\n- TPM Questions:\n  - How do you test without blocking legitimate calls?\n  - Fraud team wants 100% accuracy. Eng team says 95% max. How do you align?\n  - Launch phases: Shadow mode (90 days) → Block high-confidence fraud → Full rollout\n  - Operational: Who's oncall? What's SLA? (Block fraud in <500ms)\n\n#### 14. 5G Network Upgrade Rollout (Telecom Ops)\n- Upgrade 10K cell towers, replace hardware, software updates\n- Program: Field ops, network eng, vendor coordination, customer comms\n- TPM Questions:\n  - Prioritization: Urban areas first (revenue) or rural (brand reputation)?\n  - Risks: Hardware supply chain delays, labor shortages, weather\n  - Success metrics: Coverage %, call drop rate, customer complaints\n\n#### 15. Launch Self-Service Customer Portal (Telecom/MSP)\n- Replace call center for common tasks: Pay bill, upgrade plan, troubleshoot\n- System: React frontend, API gateway, billing integration, knowledge base\n- Program: 8 teams, 9 months\n- TPM Questions:\n  - Call center volume must drop 30% to hit ROI. How do you track?\n  - Customer support team fears job loss. How do you handle?\n  - Legacy billing API has no documentation. What's your approach?\n\n---\n\n### **Days 13-14: Mock Interviews + Refinement**\n\n**Practice (Final Prep):**\n\n#### 16. Mock Interview Problem 1\n**Scenario:** \"Design the program to replace our legacy payment processing system used by 5M customers\"\n- 60-min whiteboard: System + Program + Risks + Timeline\n\n#### 17. Mock Interview Problem 2\n**Scenario:** \"You're launching AI Copilot feature across Google Workspace (Docs, Sheets, Slides). Design the program.\"\n- Focus on: Cross-team dependencies, phased rollout, success metrics\n\n#### 18. Mock Interview Problem 3\n**Scenario:** \"You're 6 months into an 18-month platform migration. 3 teams are behind schedule and blocking 10 other teams. What do you do?\"\n- Crisis management, re-sequencing, stakeholder communication\n\n---\n\n## **How to Practice (Critical)**\n\n### **Self-Practice Framework (45 min per problem):**\n\n1. **Read problem (2 min)**\n2. **Clarify scope (3 min)** - Write down assumptions\n3. **Draw system diagram (10 min)** - High-level components only\n4. **Identify teams + dependencies (10 min)** - RACI matrix or dependency graph\n5. **Create phased rollout plan (10 min)** - Milestones, timelines\n6. **Identify top 5 risks (5 min)** - Technical, org, timeline\n7. **Define success metrics (5 min)** - How do you know it worked?\n\n### **Record Yourself:**\n- Use phone camera or Zoom record\n- Watch playback: Are you structured? Do you ramble?\n- Goal: Complete problem in 45 min, leave 15 min for interviewer questions\n\n### **Evaluation Checklist (Score yourself):**\n- [ ] Clarified ambiguous requirements upfront\n- [ ] Drew clear system diagram\n- [ ] Identified all cross-team dependencies\n- [ ] Created realistic timeline with phases\n- [ ] Spotted non-obvious risks (political, compliance, cost)\n- [ ] Defined measurable success criteria\n- [ ] Discussed trade-offs explicitly (\"We could do X but it risks Y\")\n\n---\n\n## **Resources (Free)**\n\n- **System Design Basics:** ByteByteGo YouTube (Alex Xu)\n- **TPM Program Design:** Google \"TPM interview questions GitHub\" (see real examples)\n- **Mock Interviews:** Use ChatGPT/Claude to roleplay interviewer, ask follow-ups\n\n---\n\n## **Daily Practice Schedule**\n\n### Week 1\n- **Day 1:** Study system components (3-4 hours) + Problem 1\n- **Day 2:** Problems 2-3\n- **Day 3:** Problems 4-5\n- **Day 4:** Problem 6 + review Week 1 progress\n- **Day 5:** Problem 7\n- **Day 6:** Problem 8\n- **Day 7:** Problem 9 + Week 1 self-assessment\n\n### Week 2\n- **Day 8:** Problem 10\n- **Day 9:** Problem 11\n- **Day 10:** Problem 12 + review common mistakes\n- **Day 11:** Problem 13\n- **Day 12:** Problems 14-15\n- **Day 13:** Mock Interview Problems 16-17\n- **Day 14:** Mock Interview Problem 18 + final review\n\n---\n\n## **What Success Looks Like After 2 Weeks:**\n\n✅ You can whiteboard any system design in 15 min (conceptual level)  \n✅ You can sequence a multi-team program into phases with risks identified  \n✅ You can answer \"What if X team is blocked?\" without panic  \n✅ You speak in trade-offs: \"We could launch fast but sacrifice quality\" or \"We could do it right but need 3 more months\"  \n✅ You don't just describe the system—you describe how to **build and launch it across orgs**\n\n---\n\n## **Notes**\n\n- Start with problems 1-3 on Day 1-2\n- Do 2-3 problems per day minimum\n- By Day 14, you'll be ready for Principal TPM interviews\n- Focus on **thinking out loud** - interviewers want to see your thought process\n- Don't memorize solutions - learn the frameworks and apply them\n",
    "sourceFile": "principal_tpm_system_design_2week_plan.md"
  },
  {
    "slug": "llms-principal-tpm-work-and-vp-narratives",
    "title": "LLMs, Principal TPM Work, and VP Narratives",
    "date": "2026-01-06",
    "content": "# great, give me this answer alone in a markdown file.\n\nBelow is your previous answer formatted as a standalone Markdown file.\n\n```markdown\n# LLMs, Principal TPM Work, and VP Narratives\n\nYou’re half-right, and the dangerous part is the half that’s wrong.\n\n## What LLMs Actually Change\n\n- LLMs absolutely nuke the “mechanical writing” moat. A smart PM/TPM can now turn structured inputs into a clean 1-pager, PRD, or exec email in minutes instead of hours.[web:17][web:20]  \n- So yes: grinding 3 hours on wording, formatting, and basic structure is no longer a differentiator. Treat that as dead leverage; use LLMs for the scaffolding.[web:23]\n\n## What They Do *Not* Replace\n\n- The hard part of a VP narrative is not the prose; it’s the thinking:\n  - What problem *matters* at VP level?\n  - Which tradeoffs are you foregrounding vs hiding?\n  - What you’re **willing** to commit to and what risks you’re calling out (or conveniently omitting).  \n  LLMs can’t own that judgment; they mirror what you feed them.[web:23][web:29]\n\n- Principal TPM job descriptions explicitly anchor on:\n  - “Crisp, clear written and verbal communication that aligns leaders and engineers.”\n  - “Defining the program mission, vision, tenets, and driving strategy across orgs.”[web:18][web:30]  \n  That is situational judgment plus political sense, not template-filling.\n\nIf you outsource the thinking and only keep “orchestration,” you’re indistinguishable from every mid-level PM who discovered prompts in 2024.[web:26]\n\n## Where Your “AI Orchestrator” Framing Is Right\n\nYou *should* be an intelligence orchestrator, but in a specific way:\n\n- Use LLMs to:\n  - Draft first-pass narratives, options docs, and variants.\n  - Summarize logs, incidents, and user feedback.\n  - Stress-test your docs: “attack this from the CFO’s perspective,” “rewrite from SRE’s view,” etc.[web:17][web:23]\n\n- Then you do the non-automatable work:\n  - Decide which story is politically viable.\n  - Choose the sharp, uncomfortable risk call-outs.\n  - Decide what to *not* say in a VP forum and what hill to die on.[web:25]\n\nThat second part is the Principal bar. AI can’t cover for bad judgment or lack of spine.[web:25][web:31]\n\n## How to Strengthen Your Position (Pragmatic, Not Fluffy)\n\nTreat LLM as compiler, you as architect:\n\n1. **Codify your thinking, don’t hand-wave it**  \n   For each VP 1-pager on the fraud platform, explicitly write (even in bullet notes):\n   - Problem framing (business pain, who owns it, revenue/risk impact).\n   - 2–3 sharply contrasted options with tradeoffs.\n   - Your recommendation and what you’re explicitly *not* doing.  \n   Then let LLM turn that into polished prose. You keep the thinking muscle; AI just handles the typing.[web:23]\n\n2. **Make “LLM-boosted narrative” a visible skill, not a hidden crutch**  \n   In interviews and docs, you want to implicitly show:\n   - You understand how AI changes workflows (faster iteration, more experiments, better log analysis).\n   - You still lead on vision, risk, and ethics for AI systems.  \n   That’s exactly the “AI orchestrator” PM profile top companies are leaning into.[web:14][web:17]\n\n3. **Drill live, unscripted explanation**  \n   VPs don’t care if you can submit a nice-looking PDF. They care if, when they say “Explain this fraud platform in 90 seconds,” you deliver a clear, confident narrative without ChatGPT in the room.[web:22][web:28]  \n   - Practice 3 variants of a 60–90 sec fraud story: for VP Eng, for Head of Risk, for CFO.\n   - No tools, just you. If that muscle is weak, you’re not at Principal bar regardless of how good your AI-assisted doc looks.[web:28]\n\n4. **Show you can *set* the LLM up, not just push a button**  \n   A strong Principal in this era:\n   - Designs the doc templates, review checklists, and prompting patterns the org uses.\n   - Understands where AI is safe to use (summaries, drafts) and where it’s dangerous (policy decisions, data misinterpretation).[web:14][web:21]  \n   That’s “intelligence orchestrator” with teeth, not “guy who pastes bullets into Claude.”\n\n## Blunt Truth\n\n- If your attitude is “LLM can write this, so me doing it adds no value,” you’re implicitly saying your value *was* wordsmithing, not thinking. That’s a mid-level trap.[web:28]  \n- The Principal move is: “LLM lets me externalize the busywork so I can go 3 levels deeper on strategy, risk, and org alignment than my peers.”[web:17]\n\nUse LLMs aggressively, but never let them be a substitute for having a sharp, defensible point of view. If you want, next step is simple: you dump your raw bullets for one VP narrative, and it can be turned into a 1-pager *plus* a 60-second spoken version you can deliver cold.[web:23]\n```\n\n",
    "sourceFile": "great, give me this answer alone in a markdown fil.md"
  }
];

export function getScratchPadDoc(slug: string): ScratchPadDoc | undefined {
  return scratchPadDocs.find(doc => doc.slug === slug);
}

export function getAllScratchPadSlugs(): string[] {
  return scratchPadDocs.map(doc => doc.slug);
}
